{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d62eb66-d22f-4da5-b9b7-f0d0f3963c08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Silver Layer\n",
    "In this notebook, we will create the Silver Layer of our Data Pipeline. In this layer, it is expected that the Analysts can already work with the data, given that is has already been curated, passed by a first pass of cleanup and had the data types validated.\n",
    "\n",
    "In this Layer, we will:\n",
    "* Gather the Bronze datasets and make the stocks datasets as two consolidated ones: (1) Stocks; (2) Economic Indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43b9728-1218-4448-b758-85c786460612",
     "showTitle": true,
     "title": "Imports"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing needed Pyspark functions\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, DateType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e9bc3d-ee0f-47dc-9d66-9b7ce53d09b0",
     "showTitle": true,
     "title": "Function to save to Silver Folder"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to save datasets to the Silver Layer\n",
    "def save_silver(df, name):\n",
    "    '''Save dataset to the silver folder\n",
    "    Inputs:\n",
    "    name: str = name of the folder in the silver layer'''\n",
    "\n",
    "    # Build path in the Silver Layer\n",
    "    file_path = f'/FileStore/tables/silver/{name}'\n",
    "\n",
    "    df.coalesce(1).write.format('parquet').mode('overwrite').save(file_path)\n",
    "    print(f'File {name} saved successfuly.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9ff8777-b6d9-4652-b698-8744e462823f",
     "showTitle": true,
     "title": "Create Empty Dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty RDD\n",
    "emp_RDD = spark.sparkContext.emptyRDD()\n",
    " \n",
    "# Create an expected schema\n",
    "columns = StructType([StructField('date', DateType(), True),\n",
    "                      StructField('ticker', StringType(), True),\n",
    "                      StructField('open', DoubleType(), True),\n",
    "                      StructField('high', DoubleType(), True),\n",
    "                      StructField('low', DoubleType(), True),\n",
    "                      StructField('close', DoubleType(), True),\n",
    "                      StructField('volume', IntegerType(), True),\n",
    "                      StructField('RSI', DoubleType(), True)\n",
    "                      ])\n",
    " \n",
    "# Create an empty RDD with expected schema that will hold the consolidated data\n",
    "stocks = spark.createDataFrame(data = emp_RDD,\n",
    "                           schema = columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc784443-bfd6-44a6-b840-d0061caa6dc1",
     "showTitle": true,
     "title": "Append stocks datasets in a consolidated table"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHTR added to dataframe\nCMCSA added to dataframe\nT added to dataframe\nTMUS added to dataframe\nVZ added to dataframe\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine the datasets to be united\n",
    "tickers = ['CHTR' , 'CMCSA', 'T', 'TMUS', 'VZ']\n",
    "\n",
    "# Gather transformed datasets\n",
    "for ticker in tickers:\n",
    "    path = f'/FileStore/tables/bronze/{ticker}' \n",
    "    dtf = spark.read.parquet(path)\n",
    "    stocks = stocks.union(dtf).sort('ticker', 'date')\n",
    "    print(f'{ticker} added to dataframe')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b56e6b-47a8-4e74-a9eb-44684158996a",
     "showTitle": true,
     "title": "Adding Moving Averages to the data"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adding Moving Averages to the data\n",
    "# Window Specs\n",
    "ws = Window.partitionBy('ticker').orderBy('date')\n",
    "ws7 = Window.partitionBy('ticker').orderBy('date').rowsBetween(-6, Window.currentRow )\n",
    "ws14 = Window.partitionBy('ticker').orderBy('date').rowsBetween(-13, Window.currentRow )\n",
    "ws23 = Window.partitionBy('ticker').orderBy('date').rowsBetween(-22, Window.currentRow )\n",
    "ws180 = Window.partitionBy('ticker').orderBy('date').rowsBetween(-179, Window.currentRow )\n",
    "\n",
    "\n",
    "# Add moving averages\n",
    "stocks_silver = (\n",
    "    stocks\n",
    "    .withColumn('ma7', F.mean('close').over(ws7)) # Add 7 days moving average\n",
    "    .withColumn('ma23', F.mean('close').over(ws23)) # Add 23 days moving average\n",
    "    .withColumn('ma180', F.mean('close').over(ws180)) # Add 300 days moving average\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "970fc8a0-845c-40ff-8e3d-00d3271787af",
     "showTitle": true,
     "title": "Save consolidated dataset"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File stocks saved successfuly.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save dataset\n",
    "save_silver(stocks_silver, 'stocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "849d3256-3e9f-47d7-97b5-870614923ff2",
     "showTitle": true,
     "title": "Consolidate Economic indicators"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFLATION added to dataframe\nREAL_GDP_PER_CAPITA added to dataframe\nCPI added to dataframe\n"
     ]
    }
   ],
   "source": [
    "# Create an empty RDD\n",
    "emp_RDD = spark.sparkContext.emptyRDD()\n",
    " \n",
    "# Create an expected schema\n",
    "columns = StructType([StructField('indicator', StringType(), True),\n",
    "                      StructField('value', DoubleType(), True),\n",
    "                      StructField('date', DateType(), True)\n",
    "                      ])\n",
    " \n",
    "# Create an empty RDD with expected schema that will hold the consolidated data\n",
    "indicators = spark.createDataFrame(data = emp_RDD,\n",
    "                           schema = columns) \n",
    "\n",
    "\n",
    "# Determine the datasets to be united\n",
    "tickers = ['INFLATION', 'REAL_GDP_PER_CAPITA', 'CPI']\n",
    "\n",
    "# Gather transformed datasets\n",
    "for ticker in tickers:\n",
    "    path = f'/FileStore/tables/bronze/{ticker}' \n",
    "    dtf = spark.read.parquet(path)\n",
    "    indicators = indicators.union(dtf)\n",
    "    print(f'{ticker} added to dataframe')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f069ce8b-5c67-4bde-a913-b6f412c20efd",
     "showTitle": true,
     "title": "Save consolidated dataset"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File indicators saved successfuly.\n"
     ]
    }
   ],
   "source": [
    "# Save consolidated indicators to Silver Folder\n",
    "save_silver(indicators, 'indicators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b68640-1e72-40ae-86de-6ac3b2f1d972",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File DJUSTL saved successfuly.\n"
     ]
    }
   ],
   "source": [
    "# Open ETF dataset\n",
    "etf = spark.read.parquet('/FileStore/tables/bronze/DJUSTL/')\n",
    "\n",
    "# Update columns\n",
    "etf = (etf\n",
    "       .select('date', 'ticker', 'open', 'high', 'low', 'close', 'volume')\n",
    "       .withColumn('RSI', F.lit(0))       \n",
    "       )\n",
    "\n",
    "# Save ETF DJUSTL to Silver Layer\n",
    "save_silver(etf, 'DJUSTL')\n",
    "#dbutils.fs.rm('FileStore/tables/bronze/DJUSTL/', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36939704-acc9-4e97-bb9f-c58cf129ac18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3373657055778165,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Silver Layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
