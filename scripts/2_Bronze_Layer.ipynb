{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d62eb66-d22f-4da5-b9b7-f0d0f3963c08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer\n",
    "In this notebook, we will create the Bronze Layer of our Data Pipeline. We already have fetched the data from the APIs, so now we will perform the initial cleanups.\n",
    "* Insert the *ticker* column for each of the datasets\n",
    "* Transform the date column from string to datetime\n",
    "* Drop any data before 2019-01-01, as we want to focus on the last 5 years of data.\n",
    "* Union all the data together in a single table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43b9728-1218-4448-b758-85c786460612",
     "showTitle": true,
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "# Importing needed Pyspark functions\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, count, sum, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8857e348-ebd8-46e6-a027-381dccf1004d",
     "showTitle": true,
     "title": "Function to transform the dataset"
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(ticker):\n",
    "    '''\n",
    "    Function that (1) loads a dataset; \n",
    "    (2) adds a ticker column for the stock dataset; \n",
    "    (3) Transform the column to datetime; \n",
    "    (4) Drop data before 2019-01-01\n",
    "    Inputs:\n",
    "    * ticker: str = code to be added to all obervations\n",
    "    '''\n",
    "\n",
    "    # File path to be loaded\n",
    "    path = f'/FileStore/tables/raw/{ticker}'\n",
    "\n",
    "    # As the dataset for the ETF \"DJUSTL\" was pulled from a different API, it has the column date named as datetime. It needs to be corrected\n",
    "    if ticker == \"DJUSTL\":\n",
    "        df = (\n",
    "            spark.read.parquet(path)\n",
    "            .withColumn('ticker', F.lit(ticker))\n",
    "            .withColumnRenamed('datetime', 'date')\n",
    "        )\n",
    "    else:    \n",
    "        # (2) Add ticker column\n",
    "        df = (\n",
    "            spark.read.parquet(path)\n",
    "            .withColumn('ticker', F.lit(ticker))\n",
    "        )\n",
    "\n",
    "    # Steps (3) and (4)\n",
    "    # If working with economic indices, adapt for less columns\n",
    "    if ticker in ['INFLATION', 'REAL_GDP_PER_CAPITA', 'CPI']:\n",
    "        df = (df\n",
    "            .select( 'ticker', 'value',\n",
    "                      F.to_date('date').alias('date')  )\n",
    "            .filter( col('date') >= '2019-01-01' ) #data cleanup, drop data before 2019\n",
    "          )\n",
    "    else:\n",
    "        df = (df\n",
    "            .select( 'ticker', 'open', 'high', 'low', 'close', 'volume',\n",
    "                      F.to_date('date').alias('date')  )\n",
    "            .filter( col('date') >= '2019-01-01' ) #data cleanup, drop data before 2019\n",
    "          )\n",
    "\n",
    "    # Return transformed data    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc129339-f6e4-44c3-9858-7e2684b9fdd1",
     "showTitle": true,
     "title": "Function to add RSI to the dataset"
    }
   },
   "outputs": [],
   "source": [
    "# Function to add RSI to the stock dataset\n",
    "def add_RSI(df, ticker):\n",
    "    '''Function to add RSI column to the stocks\n",
    "    inputs:\n",
    "    ticker: str = stock code'''\n",
    "\n",
    "    # File path to be loaded\n",
    "    path = f'/FileStore/tables/raw/RSI/{ticker}'\n",
    "    rsi = spark.read.parquet(path)\n",
    "\n",
    "    # Transform date to datetime format\n",
    "    rsi = (rsi\n",
    "            .select( F.to_date('date').alias('date'),\n",
    "                    col('value').alias('RSI')  )\n",
    "            .filter( col('date') >= '2019-01-01' ) #data cleanup, drop data before 2019\n",
    "    )\n",
    "\n",
    "    # Add RSI to the dataset\n",
    "    df = (df\n",
    "          .join(rsi, on='date', how= 'inner')\n",
    "          )\n",
    "\n",
    "    # Return transformed data\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37ed818-e2bc-45f9-861f-f721334edf88",
     "showTitle": true,
     "title": "Transform dataset for all folders"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHTR transformed\nCMCSA transformed\nT transformed\nTMUS transformed\nVZ transformed\nINFLATION transformed\nREAL_GDP_PER_CAPITA transformed\nCPI transformed\nDJUSTL transformed\n"
     ]
    }
   ],
   "source": [
    "names = ['CHTR' , 'CMCSA', 'T', 'TMUS', 'VZ']\n",
    "\n",
    "# Save transformed datasets\n",
    "for folder in names:\n",
    "    stock = transform_data(ticker = folder)\n",
    "    stock_bronze = add_RSI(stock, ticker=folder)\n",
    "    path = f'/FileStore/tables/bronze/{folder}' \n",
    "    stock_bronze.coalesce(1).write.format('parquet').mode('overwrite').save(path)\n",
    "    print(f'{folder} transformed')\n",
    "\n",
    "\n",
    "indicators = ['INFLATION', 'REAL_GDP_PER_CAPITA', 'CPI', 'DJUSTL']\n",
    "# Save transformed datasets\n",
    "for folder in indicators:\n",
    "    df_ind = transform_data(ticker = folder)\n",
    "    path = f'/FileStore/tables/bronze/{folder}' \n",
    "    df_ind.coalesce(1).write.format('parquet').mode('overwrite').save(path)\n",
    "    print(f'{folder} transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae639ae3-76ee-404e-9990-8643b750e3e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Bronze Layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
