{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446d5ae3-f58b-44c2-9149-ac8649e9529b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import psycopg to send data to Postgresql\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "147283e6-2761-4137-bf73-c75d0b35889a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db322d9f-21e7-45b6-822e-bd287c5e2aa9",
     "showTitle": true,
     "title": "Function for bulk insert PostgreSQL"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a function for bulk insertion in SQL PostgreSQL\n",
    "def bulk_insert(table, records):\n",
    "    try:\n",
    "        # Create a connection to the database\n",
    "        connection = psycopg2.connect(CONNECTION)\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        if table == 'monthly':\n",
    "            sql_insert_query = f\"\"\" INSERT INTO {table} (ticker, year, month, open, high, low, close, volume)\n",
    "                                    VALUES (%s,%s,%s,%s,%s,%s,%s,%s) \"\"\"\n",
    "        elif table == 'weekly':\n",
    "            sql_insert_query = f\"\"\" INSERT INTO {table} (ticker, year, week, open, high, low, close, volume)\n",
    "                                    VALUES (%s,%s,%s,%s,%s,%s,%s,%s) \"\"\"\n",
    "        elif table == 'indicators':\n",
    "            sql_insert_query = f\"\"\" INSERT INTO {table} (indicator, value, date)\n",
    "                                    VALUES (%s,%s,%s) \"\"\"\n",
    "        elif table == 'stocks':\n",
    "            sql_insert_query = f\"\"\" INSERT INTO {table} (date, ticker, open, high, low, close, volume, RSI, ma7, ma23, ma180)\n",
    "                                    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s) \"\"\"\n",
    "        else:\n",
    "            sql_insert_query = f\"\"\" INSERT INTO {table} (date, ticker, open, high, low, close, volume, RSI)\n",
    "                                    VALUES (%s,%s,%s,%s,%s,%s,%s,%s) \"\"\"\n",
    "\n",
    "        # executemany() to insert multiple rows\n",
    "        result = cursor.executemany(sql_insert_query, records)\n",
    "        connection.commit()\n",
    "        print(cursor.rowcount, \"Record inserted successfully into mobile table\")\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Failed inserting record into the table {error}\")\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "\n",
    "\n",
    "# Dataframe to list of tuples for SQL INSERT\n",
    "def df_to_tuple(df):\n",
    "    \"Convert a Spark dataframe to a list of tuples\"\n",
    "    # convert dataframe to rdd \n",
    "    rdd = df.rdd   \n",
    "    # convert rdd to tuple \n",
    "    to_tuple = rdd.map(tuple).collect()  \n",
    "\n",
    "    return to_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c69eaf-8a30-46b8-8027-da6ab975d592",
     "showTitle": true,
     "title": "Create tables in PostgreSQL"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create table in PostgreSQL\n",
    "def create_table(name, sql):\n",
    "    \"This function opens a connection with PostgreSQL and creates a table based on the sql command provided\"\n",
    "    \n",
    "    # Create a connection and cursor\n",
    "    conn = psycopg2.connect(CONNECTION)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execture SQL Commmands\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    # Commit and close connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return \"Table created\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "084aedf5-cdd9-4c05-935b-b4db604734f5",
     "showTitle": true,
     "title": "Create Monthly Table"
    }
   },
   "outputs": [],
   "source": [
    "# SQL command 1 for Monthly Data Table\n",
    "sql = f'''\n",
    "    CREATE TABLE monthly (\n",
    "    ticker VARCHAR(255),\n",
    "    year INT,\n",
    "    month INT,\n",
    "    open DECIMAL(10,2),\n",
    "    high DECIMAL(10,2),\n",
    "    low DECIMAL(10,2), \n",
    "    close DECIMAL(10,2), \n",
    "    volume INT\n",
    "   );\n",
    "'''\n",
    "\n",
    "# Creating the table\n",
    "create_table(name='monthly', sql= sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d999301f-9d18-485d-a1ef-d98870e46430",
     "showTitle": true,
     "title": "Load Monthly to PostgreSQL"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305 Record inserted successfully into mobile table\nPostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Loading monthly stock dataset\n",
    "dtf = spark.read.parquet('/FileStore/tables/gold/monthly_stocks')\n",
    "\n",
    "# Converto to list of tuple for SQL loading\n",
    "records_to_insert = df_to_tuple(dtf)\n",
    "\n",
    "# Load to PostgreSQL\n",
    "bulk_insert(table='monthly', records= records_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d28d96e9-c9b9-49e4-a53d-067dfcdcc81b",
     "showTitle": true,
     "title": "Create Weekly Table"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a table in PostgreSQL\n",
    "# SQL command 2 for Weekly Data Table\n",
    "sql2 = f'''\n",
    "    CREATE TABLE weekly (\n",
    "    ticker VARCHAR(255),\n",
    "    year INT,\n",
    "    week INT,\n",
    "    open DECIMAL(10,2),\n",
    "    high DECIMAL(10,2),\n",
    "    low DECIMAL(10,2), \n",
    "    close DECIMAL(10,2), \n",
    "    volume INT\n",
    "   );\n",
    "'''\n",
    "\n",
    "# Creating the table\n",
    "create_table(name='weekly', sql= sql2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5ba9cc-9114-434d-842e-93496e5ff123",
     "showTitle": true,
     "title": "Load Weekly to PostgreSQL"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305 Record inserted successfully into mobile table\nPostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Loading monthly stock dataset\n",
    "dtf = spark.read.parquet('/FileStore/tables/gold/weekly_stocks')\n",
    "\n",
    "# Converto to list of tuple for SQL loading\n",
    "records_to_insert = df_to_tuple(dtf)\n",
    "\n",
    "# Load to PostgreSQL\n",
    "bulk_insert(table='weekly', records= records_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd1d8d8-27fd-412f-ad28-b1136558e126",
     "showTitle": true,
     "title": "Create Stocks Table"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Table created'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a table in PostgreSQL\n",
    "# SQL command 3 for stocks Data Table\n",
    "sql3 = f'''\n",
    "    CREATE TABLE stocks (\n",
    "    date DATE,\n",
    "    ticker VARCHAR(255),\n",
    "    open DECIMAL(10,2),\n",
    "    high DECIMAL(10,2),\n",
    "    low DECIMAL(10,2), \n",
    "    close DECIMAL(10,2), \n",
    "    volume INT,\n",
    "    RSI DECIMAL(10,2),\n",
    "    ma7 DECIMAL(10,2),\n",
    "    ma23 DECIMAL(10,2),\n",
    "    ma180 DECIMAL(10,2)\n",
    "   );\n",
    "'''\n",
    "\n",
    "# Creating the table\n",
    "create_table(name='stocks', sql= sql3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f399111d-7a2e-4358-870b-f6da624c6a33",
     "showTitle": true,
     "title": "Load Stocks to PostgreSQL"
    }
   },
   "outputs": [],
   "source": [
    "# Loading monthly stock dataset\n",
    "dtf = spark.read.parquet('/FileStore/tables/silver/stocks')\n",
    "\n",
    "# Converto to list of tuple for SQL loading\n",
    "records_to_insert = df_to_tuple(dtf)\n",
    "\n",
    "# Load to PostgreSQL\n",
    "bulk_insert(table='stocks', records= records_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c03469-a551-4281-a22e-d087d1bb23fb",
     "showTitle": true,
     "title": "Create Indicators Table"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a table in PostgreSQL\n",
    "# SQL command 3 for economic indicators Data Table\n",
    "sql4 = f'''\n",
    "    CREATE TABLE indicators (\n",
    "    indicator VARCHAR(255),\n",
    "    value DECIMAL(10,2),\n",
    "    date DATE\n",
    "   );\n",
    "'''\n",
    "\n",
    "# Creating the table\n",
    "create_table(name='indicators', sql= sql4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8137e528-7624-49fc-8dfb-ef94bfd73e20",
     "showTitle": true,
     "title": "Load Indicators to PostgreSQL"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 Record inserted successfully into mobile table\nPostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Loading monthly stock dataset\n",
    "dtf = spark.read.parquet('/FileStore/tables/silver/indicators')\n",
    "\n",
    "# Converto to list of tuple for SQL loading\n",
    "records_to_insert = df_to_tuple(dtf)\n",
    "\n",
    "# Load to PostgreSQL\n",
    "bulk_insert(table='indicators', records= records_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ccd8234-e997-451c-a864-1cc8108d8675",
     "showTitle": true,
     "title": "Create TELCO Index Table"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Table created'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a table in PostgreSQL\n",
    "# SQL command 3 for Telco Index Data Table\n",
    "sql5 = f'''\n",
    "    CREATE TABLE telco_idx (\n",
    "    date DATE,\n",
    "    ticker VARCHAR(255),\n",
    "    open DECIMAL(10,2),\n",
    "    high DECIMAL(10,2),\n",
    "    low DECIMAL(10,2), \n",
    "    close DECIMAL(10,2), \n",
    "    volume BIGINT,\n",
    "    RSI DECIMAL(10,2)\n",
    "   );\n",
    "'''\n",
    "\n",
    "# Creating the table\n",
    "create_table(name='telco_idx', sql= sql5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73f1c37-c833-429b-aecf-bb97d7ff11d5",
     "showTitle": true,
     "title": "Load TELCO Index to PostgreSQL"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949 Record inserted successfully into mobile table\nPostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Loading monthly stock dataset\n",
    "dtf = spark.read.parquet('/FileStore/tables/silver/DJUSTL')\n",
    "\n",
    "# Converto to list of tuple for SQL loading\n",
    "records_to_insert = df_to_tuple(dtf)\n",
    "\n",
    "# Load to PostgreSQL\n",
    "bulk_insert(table='telco_idx', records= records_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1da86ff-7839-479a-9f8c-95814965930f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Connecting with PostgreSQL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
